{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d065908-edc3-4b48-9830-c19a8fd0e167",
   "metadata": {},
   "source": [
    "# LLM工具說明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20923397-5553-4898-87d4-b33bcd370b0f",
   "metadata": {},
   "source": [
    "## llm_tools模組說明\n",
    "### 參數設定\n",
    "- 查看configs/llm.ini\n",
    "    - **DEFAULT**: 預設的參數，包含temperature、max_tokens、top_p、frequency_penalty、presence_penalty等\n",
    "    - **Azure OpenAI系列**: 設定azure_api_base、azure_api_key、azure_api_version，打了會花錢，請謹慎使用\n",
    "    - **地端LLM/Embedding Model系列**: 設定local_api_key、local_base_url，目前統一接口8887，若有實驗新模型可自行新增\n",
    "  \n",
    "### 導入方法\n",
    "- 第一步：將llm_tools資料夾放在跟操作程式碼相同路徑\n",
    "- 第二步：在python中導入以下模組\n",
    "    - from llm_tools.llm_chat import LLMChat\n",
    "    - from llm_tools.embedding_model import EmbeddingModel\n",
    "\n",
    "### 函數內容\n",
    "- llm_chat.py 重要參數與功能\n",
    "    - params (dictionary): 可控制參數\n",
    "    - response_format (string): 可設定\"json_object\"，讓LLM返回json物件，但是prompt內要有json的說明\n",
    "    - stream (bool): 設定為True可使用流式輸出\n",
    "    - history (list): OpenAI接口回傳的history，格式為[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": query}. {\"role\": \"system\", \"content\": completion}]\n",
    "- embedding_model.py 重要參數與功能\n",
    "    - 單句轉Embedding: 輸出為array\n",
    "    - 多句(list)轉Embedding: 會集合成batch方式轉譯，跑批的時候速度較快，輸出為array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49136a42-2b09-4923-b71b-f0083863a23d",
   "metadata": {},
   "source": [
    "## LLM單句對答\n",
    "- 可測stream、json回答等\n",
    "- response會把stream的回答合併起來，所以用stream模式如果print(response)會看到兩次回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9046b5-e747-4510-9517-3cf08d12737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語意主題：台積電 A16 製程及其影響\n",
      "\n",
      "內容描述：\n",
      "1. 主題：台積電在北美技術論壇上展示了埃米級（A16）製程，這是一個重要的科技發展里程碑，預計於 2026 年開始量產。\n",
      "2. 具體細節：A16 製程引發了各大科技公司的關注和競爭，特別是 OpenAI 等公司對其有興趣。\n",
      "3. 技術特點：A16 製程引入了 SPR 系統，相較於之前的 N2P 製程，在保持相同性能的同時能降低最多 20% 的功耗，并提升近 10% 的晶片密度。這顯示出更高的能源效率和更小的芯片尺寸。\n",
      "4. 目標應用：OpenAI 想要利用 A16 製程來開發自家的 AI 晶片，以增強他們的 GPT語言模型和影像生成模型 Sora的能力。\n",
      "\n",
      "總結來說，該段落主要討論的是台積電新製程 A16 的技術進步、其優勢以及對人工智能領域如 OpenAI 所帶來的可能性。\n",
      "語意主題：台積電 A16 製程及其影響\n",
      "\n",
      "內容描述：\n",
      "1. 主題：台積電在北美技術論壇上展示了埃米級（A16）製程，這是一個重要的科技發展里程碑，預計於 2026 年開始量產。\n",
      "2. 具體細節：A16 製程引發了各大科技公司的關注和競爭，特別是 OpenAI 等公司對其有興趣。\n",
      "3. 技術特點：A16 製程引入了 SPR 系統，相較於之前的 N2P 製程，在保持相同性能的同時能降低最多 20% 的功耗，并提升近 10% 的晶片密度。這顯示出更高的能源效率和更小的芯片尺寸。\n",
      "4. 目標應用：OpenAI 想要利用 A16 製程來開發自家的 AI 晶片，以增強他們的 GPT語言模型和影像生成模型 Sora的能力。\n",
      "\n",
      "總結來說，該段落主要討論的是台積電新製程 A16 的技術進步、其優勢以及對人工智能領域如 OpenAI 所帶來的可能性。\n"
     ]
    }
   ],
   "source": [
    "from llm_tools.llm_chat import LLMChat\n",
    "\n",
    "# Test LLM\n",
    "# llmchat = LLMChat(model=\"gpt-4o\")\n",
    "llmchat = LLMChat(model=\"Qwen1.5-14B-Chat\")\n",
    "# llmchat = LLMChat(model=\"Qwen2-7B-Instruct\")\n",
    "\n",
    "params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"top_p\": 1,\n",
    "    \"frequency_penalty\": 1.4,\n",
    "    \"presence_penalty\": 0\n",
    "}\n",
    "try:\n",
    "    system = \"\"\"\n",
    "    任務：解讀主題與內容\n",
    "    請從下面的文字中解讀該段落的語意主題與內容，應該詳細閱讀資訊，並提取其語意主題與描述內容。\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    咱們的護國神山，台積電在今年四月的北美技術論壇上，發表埃米級的 A16 製程，預計於 2026 年開始量產，引發各大科技龍頭的瘋搶。\n",
    "    台積的 A16 製程更引入 SPR 系統，不只減少了 IR 降壓，相較於過去的 N2P 製程，能在相同性能下，減少最多 20% 的功耗，同時提升近 10% 的晶片密度。\n",
    "    OpenAI 也希望得到 A16 製程的協助，打造屬於自家的 AI 晶片，用以強化旗下的 GPT 語言模型，以及影像生成模型 Sora。\n",
    "    \"\"\"\n",
    "    history = None\n",
    "    # response, history = llmchat.chat(query=query, system=system, params=params, response_format=\"json_object\", stream=True) # response規定json\n",
    "    response, history = llmchat.chat(query=query, system=system, stream=True)\n",
    "    print()\n",
    "    print(response)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc0231-757b-4bc4-92b3-89df7d780357",
   "metadata": {},
   "source": [
    "## 測試LLM多輪問答\n",
    "- 用while迴圈來做多輪問答看看唄\n",
    "- 輸入quit結束問答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3d663-fc8d-4474-aa9e-d9c2bca06688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入你的問題:  第一次問答\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "當然，我很樂意回答您的問題。請問您有什麼想要了解的呢？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入你的問題:  第二次問答\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非常好，如果您有任何问题，无论是关于学术、科技、生活常识还是其他任何主题，请随时向我提问。我会尽力提供准确和有用的信息。那么请问您有什么想要了解的内容呢？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入你的問題:  第三次問答\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然，很高兴再次为您服务。请问您有什么问题或需要讨论的主题吗？无论是关于学习、工作、生活建议，还是任何其他领域的问题，我都将尽力提供帮助。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入你的問題:  我問了幾次?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您已经进行了三次问答回合。每次问答回合包括一次初始询问和两次后续回应，因此总共是三次问答回合。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入你的問題:  quit\n"
     ]
    }
   ],
   "source": [
    "from llm_tools.llm_chat import LLMChat\n",
    "\n",
    "# Test LLM Chatbot\n",
    "llmchat = LLMChat(model=\"Qwen2-7B-Instruct\")\n",
    "history=None \n",
    "is_stream=True\n",
    "while True:\n",
    "    input_text = input(\"請輸入你的問題: \")  \n",
    "    if input_text.lower() == \"quit\":  \n",
    "        break\n",
    "    response, history = llmchat.chat(query=input_text, history=history, stream=is_stream)\n",
    "    if is_stream==True:\n",
    "        print()\n",
    "    else:\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f1ada-6b70-424b-ad55-3bfef5965142",
   "metadata": {},
   "source": [
    "## Embedding測試\n",
    "- 單句轉Embedding與多句轉Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b968e1c4-aa2e-4219-af98-96f9ca8c6066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "(2, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from llm_tools.embedding_model import EmbeddingModel\n",
    "\n",
    "# Test EmbeddingModel\n",
    "embmodel = EmbeddingModel(embedding_model=\"m3e-base\")\n",
    "query = \"The food was delicious and the waiter was friendly.\"\n",
    "query_embedding = np.array(embmodel.embed_query(query))\n",
    "print(len(query_embedding))\n",
    "documents = [\"The food was delicious and the waiter was friendly.\",\n",
    "             \"The service was slow and the food was not very good.\"]\n",
    "document_embeddings = np.array(embmodel.embed_documents(documents))\n",
    "print(document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81425f-9402-475d-bb86-a1edf18af995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
